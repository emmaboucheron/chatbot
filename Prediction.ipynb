{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ETIENNE\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Importation des librairies utilisées\n",
    "import unicodedata \n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "import collections\n",
    "import itertools\n",
    "import csv\n",
    "import warnings\n",
    "\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Répertoire de travail\n",
    "DATA_DIR = \"C:/Users/ETIENNE/Documents/Work/INSA/4A/Projets 4gmm 2018/\"\n",
    "\n",
    "# Nom des fichiers\n",
    "training_reduit_path = DATA_DIR + \"INSA_wefight_data_clean.csv\"\n",
    "# Variable Globale\n",
    "HEADER_TEST = ['Question','Intent','BlockId', 'Action']\n",
    "HEADER_TRAIN =['Question','Intent','BlockId', 'Action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Takes 0 s\n"
     ]
    }
   ],
   "source": [
    "def split_dataset(input_path, nb_line, tauxValid,columns):\n",
    "    time_start = time.time()\n",
    "    data_all = pd.read_csv(input_path,sep=\",\",names=columns,nrows=nb_line) #cree data frame\n",
    "    data_all = data_all.fillna(\"\") #remplace les na par \" \"\n",
    "    data_train, data_valid = train_test_split(data_all, test_size = tauxValid) # Split arrays or matrices into random train and test subsets\n",
    "    time_end = time.time()\n",
    "    print(\"Split Takes %d s\" %(time_end-time_start))\n",
    "    return data_train, data_valid\n",
    "\n",
    "nb_line=20000  # part totale extraite du fichier initial ici déjà réduit\n",
    "tauxValid=0.10 # part totale extraite du fichier initial ici déjà réduit\n",
    "data_train, data_valid = split_dataset(training_reduit_path, nb_line, tauxValid, HEADER_TRAIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Librairies \n",
    "from bs4 import BeautifulSoup #Nettoyage d'HTML\n",
    "import re # Regex\n",
    "import nltk # Nettoyage des données\n",
    "\n",
    "## listes de mots à supprimer dans la description des produits\n",
    "## Depuis NLTK\n",
    "nltk_stopwords = nltk.corpus.stopwords.words('french') \n",
    "## Depuis Un fichier externe.\n",
    "lucene_stopwords = [unicode(w, \"utf-8\") for w in open(DATA_DIR+\"lucene_stopwords.txt\").read().split(\",\")] #En local\n",
    "\n",
    "## Union des deux fichiers de stopwords \n",
    "stopwords = list(set(nltk_stopwords).union(set(lucene_stopwords)))\n",
    "\n",
    "## Fonction de setmming de stemming permettant la racinisation\n",
    "stemmer=nltk.stem.SnowballStemmer('french')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fonction clean générale\n",
    "def clean_txt(txt):\n",
    "    ### remove html stuff\n",
    "    txt = BeautifulSoup(txt,\"html.parser\",from_encoding='utf-8').get_text() #nettoyage donnee html\n",
    "    ### lower case\n",
    "    txt = txt.lower()\n",
    "    ### special escaping character '...'\n",
    "    txt = txt.replace(u'\\u2026','.')\n",
    "    txt = txt.replace(u'\\u00a0',' ')\n",
    "    ### remove accent btw\n",
    "    txt = unicodedata.normalize('NFD', txt).encode('ascii', 'ignore')\n",
    "    ###txt = unidecode(txt)\n",
    "    ### remove non alphanumeric char\n",
    "    txt = re.sub('[^a-z_]', ' ', txt)\n",
    "    ### remove french stop words\n",
    "    tokens = [w for w in txt.split() if (len(w)>2) and (w not in stopwords)]\n",
    "    ### french stemming\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    #Stemmers remove morphological affixes from words, leaving only the word stem\n",
    "    ### tokens = stemmer.stemWords(tokens)\n",
    "    return ' '.join(tokens)\n",
    "    #join() returns a string in which the string elements of sequence have been joined by str separator.\n",
    "\n",
    "def clean_marque(txt):\n",
    "    txt = re.sub('[^a-zA-Z0-9]', '_', txt).lower()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fonction de nettoyage du fichier(stemming et liste de mots à supprimer)\n",
    "def clean_df(input_data, column_names= ['Question','Intent','BlockId', 'Action']):\n",
    "    #Test if columns entry match columns names of input data\n",
    "    column_names_diff= set(column_names).difference(set(input_data.columns))\n",
    "    #set.difference   new set with elements in column_names but not in input_data.columns\n",
    "    \n",
    "    if column_names_diff: #rentre dans la boucle si column_names différent zero\n",
    "        # warning = exception\n",
    "        warnings.warn(\"Column(s) '\"+\", \".join(list(column_names_diff)) +\"' do(es) not match columns of input data\", Warning)\n",
    "        \n",
    "    nb_line = input_data.shape[0]\n",
    "    print(\"Start Clean %d lines\" %nb_line)\n",
    "    \n",
    "    # Cleaning start for each columns\n",
    "    time_start = time.time()\n",
    "    clean_list=[]\n",
    "    for column_name in column_names:\n",
    "        column = input_data[column_name].values\n",
    "        if column_name == \"Question\":\n",
    "            array_clean = np.array(map(clean_txt,column))\n",
    "            \n",
    "        elif column_name == \"Intent\":\n",
    "            array_clean = np.asarray(input_data['Intent']) #on recopie telle quelle la colonne intent  \n",
    "            \n",
    "        else:\n",
    "            array_clean = np.array(map(clean_marque,column))\n",
    "            #applies a function to all the items in an input_list\n",
    "            #map(function_to_apply, list_of_inputs)\n",
    "        clean_list.append(array_clean)\n",
    "    time_end = time.time()\n",
    "    print(\"Cleaning time: %d secondes\"%(time_end-time_start))\n",
    "    \n",
    "    #Convert list to DataFrame\n",
    "    array_clean = np.array(clean_list).T\n",
    "    data_clean = pd.DataFrame(array_clean, columns = column_names)\n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Clean 502 lines\n",
      "Cleaning time: 0 secondes\n",
      "Start Clean 4511 lines\n",
      "Cleaning time: 4 secondes\n"
     ]
    }
   ],
   "source": [
    "# Take approximately 2 minutes fors 100.000 rows\n",
    "data_valid_clean = clean_df(data_valid)\n",
    "data_train_clean = clean_df(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data_valid_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import FeatureHasher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizer_train(df,column):\n",
    "    col = df[column]\n",
    "    # TFIDF\n",
    "    vec = TfidfVectorizer(\n",
    "            min_df = 1, #on prend tous les mots\n",
    "            stop_words =stopwords,\n",
    "            smooth_idf=True,\n",
    "            norm='l2',\n",
    "            sublinear_tf=True,\n",
    "            use_idf=True, #tf avec idf\n",
    "            ngram_range=(1,1)) \n",
    "    tfidf=vec.fit_transform(col)\n",
    "    return vec,tfidf\n",
    "\n",
    "def apply_vectorizer(df, vec, columns):\n",
    "    \n",
    "    data_hash = map(lambda x : \" \".join(x), df[columns].values)  \n",
    "    tfidf=vec.transform(df[columns])\n",
    "\n",
    "    # TFIDF\n",
    "    #tfidf=vec.transform(df)\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec,X = vectorizer_train(data_train_clean,\"Question\")\n",
    "Y = data_train_clean[\"Intent\"].values\n",
    "Xv = apply_vectorizer(data_valid_clean,vec,\"Question\")\n",
    "Yv = data_valid_clean[\"Intent\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<502x1949 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 1417 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('# training score:', 0.79849257370871207)\n"
     ]
    }
   ],
   "source": [
    "# Regression Logistique \n",
    "## estimation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#si on augmente C, on augmente bcp le score\n",
    "cla = LogisticRegression(penalty='l2', dual=False, tol=0.0001, C=1, fit_intercept=True,\n",
    "                          intercept_scaling=1, class_weight=None, random_state=None, solver='liblinear',\n",
    "                          max_iter=100, multi_class='ovr', verbose=0, warm_start=False, n_jobs=1)\n",
    "cla.fit(X,Y)\n",
    "score=cla.score(X,Y)\n",
    "Y_predict = cla.predict(X)\n",
    "                \n",
    "print('# training score:',score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('# validation score:', 0.71713147410358569)\n"
     ]
    }
   ],
   "source": [
    "## erreur en validation\n",
    "scoreValidation=cla.score(Xv,Yv)\n",
    "predict_v = cla.predict(Xv)\n",
    "##probleme car tfidf et Xv pas le même nbre de colonne \n",
    "print('# validation score:',scoreValidation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('#6-49_TRTEINS_Peau', 43),\n",
       " ('#2-130_QVDP_Alimentation', 21),\n",
       " ('#2-36_QVDP_Alopecie_Pourquoi', 20),\n",
       " ('#6-97_TRTEINS_Nausees_Vomissements', 18),\n",
       " ('#6-1_TRTEINS_Chimiotherapie', 17),\n",
       " ('#9-2_Informations_cancer', 14),\n",
       " ('#6-92_TRTEINS_Diarrhee', 14),\n",
       " ('#2-107_QVDP_Grossesse', 14),\n",
       " ('#2-45_QVDP_Alopecie_Perruque', 12),\n",
       " ('conversation_rappel_rendezvous', 12),\n",
       " ('#2-55_QVDP_Douleur', 11),\n",
       " ('#6-90_TRTEINS_Mauvais_Gout', 11),\n",
       " ('#6-60_TRTEINS_PAC', 11),\n",
       " ('#6-41_TRTEINS_ManchonLymphodeme', 10),\n",
       " ('#6-96_TRTEINS_Perte_Poids', 10),\n",
       " ('#6-53_TRTEINS_Aphtes', 10),\n",
       " ('#5-37_Soutien_Psychologique', 9),\n",
       " ('#6-73_TRTEINS_Chirurgie_Mastectomie', 9),\n",
       " ('#6-98_TRTEINS_EI_Frequents', 9),\n",
       " ('#2-125_QVDP_Sexualite_Reconstruction', 8),\n",
       " ('#2-96_QVDP_Social_Priseencharge', 8),\n",
       " ('#6-65_TRTEINS_Chirurgie', 8),\n",
       " ('#2-64-0_QVDP_Fatigue', 8),\n",
       " ('#6-34_TRTEINS_Lymphoedeme', 8),\n",
       " ('#6-93_TRTEINS_Mauvaise_Haleine', 8),\n",
       " ('#6-57_TRTEINS_Yeux_Secs', 7),\n",
       " ('#6-18_TRTEINS_Radiotherapie', 7),\n",
       " ('#2-128_QVDP_Sexualite_Couple', 7),\n",
       " ('#2-120_QVDP_Sexualite', 7),\n",
       " ('#2-73_QVDP_Sport', 7),\n",
       " ('#9-53_Informations_depistagesein', 6),\n",
       " ('#6-31_TRTEINS_Effetsecondaireshormonotherapie', 6),\n",
       " ('#6-73_TRTEINS_Chirurgie_Curage_axillaire', 6),\n",
       " ('#6-24_TRTEINS_hormonotherapie', 6),\n",
       " ('#2-126_QVDP_Sexualite_BaisseLibido', 5),\n",
       " ('#6-95_TRTEINS_Perte_Appetit', 5),\n",
       " ('#6-58_TRTEINS_Bouche_Seche', 5),\n",
       " ('#2-97_QVDP_ResteACharge', 4),\n",
       " ('#2-58_QVDP_RadioT_Eff', 4),\n",
       " ('#7-52_Remission', 4),\n",
       " ('#6-64_TRTEINS_PAC_Risques', 4),\n",
       " ('#2-49_QVDP_Alopecie_OuPerruque', 4),\n",
       " ('#2-48_QVDP_Alopecie_RbPerruque', 4),\n",
       " ('#2-87_QVDP_Fertilite', 4),\n",
       " ('#2-42_QVDP_Alopecie_Casque', 4),\n",
       " ('#1-5_Menu_Aidant', 3),\n",
       " ('#2-106_QVDP_Social_HAD', 3),\n",
       " ('#2-121_QVDP_Sexualite_Cicatrice', 3),\n",
       " ('#2-38_QVDP_Alopecie_Repousse', 3),\n",
       " ('#9-13_Informations_metastase', 3),\n",
       " ('#6-23_TRTEINS_SeanceRadiotherapie', 3),\n",
       " ('#6-83_TRTEINS_Prothese', 3),\n",
       " ('#2-135_QVDP_Alimentation_Alcool', 2),\n",
       " ('#2-53_QVDP_Cils', 2),\n",
       " ('#2-52_QVDP_Ongles', 2),\n",
       " ('#2-137_QVDP_Alimentation_Remboursement', 2),\n",
       " ('#6-40_TRTEINS_PressotherapieLymphodeme', 2),\n",
       " ('#2-103_QVDP_Social_Banque', 2),\n",
       " ('#1-8_Menu_Traitement', 2),\n",
       " ('#2-134_QVDP_Alimentation_Citron', 2),\n",
       " ('#2-90_QVDP_Fertilite_Ponction', 2),\n",
       " ('#2-41_QVDP_Alopecie_Diminuer', 2),\n",
       " ('#2-115_QVDP_Grossesse_Allaitement', 2),\n",
       " ('#2-79_QVDP_SportQuel', 2),\n",
       " ('Conversation_rappel_read', 2),\n",
       " ('#2-109_QVDP_Grossesse_Chimiotherapie', 2),\n",
       " ('Conversation_rappel_update2', 2),\n",
       " ('#2-84_QVDP_Fertilite_GrossesseApresCancer', 2),\n",
       " ('#2-60_QVDP_RadioT_Diarrh\\xc3\\x83\\xc2\\xa9e', 1),\n",
       " ('#2-118_QVDP_Grossesse_Malformations', 1),\n",
       " ('#6-9_TRTEINS_ChimioIntraperitoneale', 1),\n",
       " ('#6-67_TRTEINS_Chirurgie_Questions', 1),\n",
       " ('#001_Gestion_Abonnement', 1),\n",
       " ('#2-70_QVDP_Fatigue_Diminuer', 1),\n",
       " ('#6-39_TRTEINS_DrainageLymphodeme', 1),\n",
       " ('#2-47_QVDP_Alopecie_CoutPerruque', 1),\n",
       " ('#8-6_DDP_Consentement_eclaire', 1),\n",
       " ('#9-20_Informations_Stades', 1),\n",
       " ('#7-58_Recidive', 1),\n",
       " ('#6-72_TRTEINS_Chirurgie_Cicatrice', 1),\n",
       " ('#2-72_QVDP_Fatigue_Sieste', 1),\n",
       " ('#2-133_QVDP_RegimeAnticancer', 1),\n",
       " ('#2-93_QVDP_Social_Proches', 1),\n",
       " ('#2-59_QVDP_RadioT_Fatigue', 1),\n",
       " ('#2-101_QVDP_Social_RbProthese', 1),\n",
       " ('#7-49_QVDA_Personne_de_confiance', 1),\n",
       " ('#8-13_DDP_Secret_medical', 1),\n",
       " ('#2-85_QVDP_Fertilite_Preserver', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(collections.Counter(predict_v).items(), key=lambda x : x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<502x1949 sparse matrix of type '<type 'numpy.float64'>'\n",
       "\twith 1417 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
