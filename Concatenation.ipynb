{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://www.math.univ-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo_imt.jpg\" style=\"float:right; max-width: 250px; display: inline\" alt=\"IMT\"/> </a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot: catégorisations et prévisions des réponses aux questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Mining et Catégorisation de Produits en <a href=\"https://www.python.org/\"><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/f8/Python_logo_and_wordmark.svg/390px-Python_logo_and_wordmark.svg.png\" style=\"max-width: 120px; display: inline\" alt=\"R\"/></a> avec <a href=\"http://scikit-learn.org/stable/#\"><img src=\"http://scikit-learn.org/stable/_static/scikit-learn-logo-small.png\" style=\"max-width: 100px; display: inline\" alt=\"R\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit de prévoir la réponse à une question à partir de son sujet. Seule la catégorie principale (1er niveau) est prédite mais nous pourrons essayer d'affiner par la suite. L'objectif est plutôt de comparer les performances des méthodes et technologies en fonction de la taille de la base d'apprentissage ainsi que d'illustrer sur un exemple complexe le prétraitement de données textuelles. La stratégie de sous ou sur échantillonnage des catégories qui permet d'améliorer la prévision va être mise en oeuvre. Nous allons essayer différentes techniques d'échantillonnage comme le regroupement de plusieurs catégories en fonction de leur points communs ou selon leur taille puis nous essayerons de mettre en place un mélange de upsampling et de downsampling.\n",
    "* Nous allons réduire l'échantillon réduit en le séparant en 2 parties: apprentissage et validation. \n",
    "* Les données textuelles sont  nettoyées, racinisées, vectorisées avant modélisation.\n",
    "* Trois modélisations sont estimées: logistique, arbre, forêt aléatoire.\n",
    "* Optimiser l'erreur en faisant varier différents paramètres: types et paramètres de vectorisation (TF-IDF), paramètres de la régression logistique (pénalisation l1) et de la forêt aléatoire (nombre d'arbres et nombre de variables aléatoire).\n",
    "\n",
    "Exécuter finalement le code pour différentes tailles (paramètre  `tauxTot` ci-dessous) de l'échantillon d'apprentissage et comparer les qualités de prévision obtenues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\emma\\Anaconda2\\lib\\site-packages\\sklearn\\cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#Importation des librairies utilisées\n",
    "import unicodedata \n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import nltk\n",
    "import collections\n",
    "import itertools\n",
    "import csv\n",
    "import warnings\n",
    "\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importation des données\n",
    "Définition du répertoir de travail, des noms des différents fichiers utilisés et des variables globales.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Répertoire de travail\n",
    "DATA_DIR = \"C:\\Users\\emma\\Documents\\ecole\\insa\\projet chabot\\INSA_wefight_data_clean.csv\"\n",
    "\n",
    "# Nom des fichiers\n",
    "training_reduit_path = DATA_DIR \n",
    "# Variable Globale\n",
    "HEADER_TEST = ['Question','Intent','BlockId', 'Action']\n",
    "HEADER_TRAIN =['Question','Intent','BlockId', 'Action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "## Si nécessaire (première exécution) chargement de nltk, librairie pour la suppression \n",
    "## des mots d'arrêt et la racinisation\n",
    "## nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ### Read & Split Dataset\n",
    "   Fonction permettant de lire le fichier d'apprentissage et de créer deux DataFrame Pandas, un pour l'apprentissage, l'autre pour la validation.\n",
    "   La première méthode créée un DataFrame en lisant entièrement le fichier. Puis elle scinde le DataFrame en deux  grâce à la fonction dédiée de sklearn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split Takes 0 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Intent</th>\n",
       "      <th>BlockId</th>\n",
       "      <th>Action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1211</th>\n",
       "      <td>le casque réfrigérant ca marche comment ?</td>\n",
       "      <td>#2-42_QVDP_Alopecie_Casque</td>\n",
       "      <td>598435b6e4b03f0d12d99ffe</td>\n",
       "      <td>wiki_cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3323</th>\n",
       "      <td>cancer du sein et salive</td>\n",
       "      <td>#6-58_TRTEINS_Bouche_Seche</td>\n",
       "      <td>59918e6ee4b0feb2888c9bcd</td>\n",
       "      <td>wiki_cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>salon de coiffure perruque</td>\n",
       "      <td>#2-49_QVDP_Alopecie_OuPerruque</td>\n",
       "      <td>598439eae4b03f0d12f637ea</td>\n",
       "      <td>wiki_cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1755</th>\n",
       "      <td>Pourquoi je souffre de l'insomnie malgré que j...</td>\n",
       "      <td>#2-64-0_QVDP_Fatigue</td>\n",
       "      <td>59942265e4b068eebf52bb9f</td>\n",
       "      <td>wiki_cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2782</th>\n",
       "      <td>Qu est ce au drainage lymphatique</td>\n",
       "      <td>#6-39_TRTEINS_DrainageLymphodeme</td>\n",
       "      <td>598b50d6e4b03f0d3717ad2d</td>\n",
       "      <td>wiki_cancer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Question  \\\n",
       "1211          le casque réfrigérant ca marche comment ?   \n",
       "3323                           cancer du sein et salive   \n",
       "1406                         salon de coiffure perruque   \n",
       "1755  Pourquoi je souffre de l'insomnie malgré que j...   \n",
       "2782                  Qu est ce au drainage lymphatique   \n",
       "\n",
       "                                Intent                   BlockId       Action  \n",
       "1211        #2-42_QVDP_Alopecie_Casque  598435b6e4b03f0d12d99ffe  wiki_cancer  \n",
       "3323        #6-58_TRTEINS_Bouche_Seche  59918e6ee4b0feb2888c9bcd  wiki_cancer  \n",
       "1406    #2-49_QVDP_Alopecie_OuPerruque  598439eae4b03f0d12f637ea  wiki_cancer  \n",
       "1755              #2-64-0_QVDP_Fatigue  59942265e4b068eebf52bb9f  wiki_cancer  \n",
       "2782  #6-39_TRTEINS_DrainageLymphodeme  598b50d6e4b03f0d3717ad2d  wiki_cancer  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_dataset(input_path, nb_line, tauxValid,columns):\n",
    "    time_start = time.time()\n",
    "    data_all = pd.read_csv(input_path,sep=\",\",names=columns,nrows=nb_line) #cree data frame\n",
    "    data_all = data_all.fillna(\"\") #remplace les na par \" \"\n",
    "    data_train, data_valid = train_test_split(data_all, test_size = tauxValid) # Split arrays or matrices into random train and test subsets\n",
    "    time_end = time.time()\n",
    "    print(\"Split Takes %d s\" %(time_end-time_start))\n",
    "    return data_train, data_valid\n",
    "\n",
    "nb_line=20000  # part totale extraite du fichier initial ici déjà réduit\n",
    "tauxValid=0.10 # part totale extraite du fichier initial ici déjà réduit\n",
    "data_train, data_valid = split_dataset(training_reduit_path, nb_line, tauxValid, HEADER_TRAIN)\n",
    "data_all=pd.read_csv(training_reduit_path,sep=\",\",names=HEADER_TRAIN,nrows=nb_line)\n",
    "# Cette ligne permet de visualiser les 5 premières lignes de la DataFrame \n",
    "data_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Nettoyage des données\n",
    "Afin de limiter la dimension de l'espace des variables ou *features*, tout en conservant les informations essentielles, il est nécessaire de nettoyer les données en appliquant plusieurs étapes:\n",
    "* Chaque mot est écrit en minuscule.\n",
    "* Les termes numériques, de ponctuation et autres symboles sont supprimés.\n",
    "* 155 mots-courants, et donc non informatifs, de la langue française sont supprimés (STOPWORDS). Ex: le, la, du, alors, etc...\n",
    "* Chaque mot est \"racinisé\", via la fonction `STEMMER.stem` de la librairie nltk. La racinisation transforme un mot en son radical ou sa racine. Par exemple, les mots: cheval, chevaux, chevalier, chevalerie, chevaucher sont tous remplacés par \"cheva\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importation des librairies et fichier pour le nettoyage des données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Librairies \n",
    "from bs4 import BeautifulSoup #Nettoyage d'HTML\n",
    "import re # Regex\n",
    "import nltk # Nettoyage des données\n",
    "\n",
    "## listes de mots à supprimer dans la description des produits\n",
    "## Depuis NLTK\n",
    "nltk_stopwords = nltk.corpus.stopwords.words('french') \n",
    "## Depuis Un fichier externe.\n",
    "lucene_stopwords = [unicode(w, \"utf-8\") for w in open(\"C:\\Users\\emma\\Documents\\ecole\\insa\\projet chabot\\lucene_stopwords.txt\").read().split(\",\")] #En local\n",
    "\n",
    "## Union des deux fichiers de stopwords \n",
    "stopwords = list(set(nltk_stopwords).union(set(lucene_stopwords)))\n",
    "\n",
    "## Fonction de setmming de stemming permettant la racinisation\n",
    "stemmer=nltk.stem.SnowballStemmer('french')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction de nettoyage de texte\n",
    "Fonction qui prend en intrée un texte et retourne le texte nettoyé en appliquant successivement les étapes suivantes: Nettoyage des données HTML, conversion en texte minuscule, encodage uniforme, suppression des caractéres non alpha numérique (ponctuations), suppression des stopwords, racinisation de chaque mot individuellement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remarque\n",
    "#'a b c'.split()\n",
    "#str.split('a b c')\n",
    "# both return ['a', 'b', 'c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Fonction clean générale\n",
    "def clean_txt(txt):\n",
    "    ### remove html stuff\n",
    "    txt = BeautifulSoup(txt,\"html.parser\",from_encoding='utf-8').get_text() #nettoyage donnee html\n",
    "    ### lower case\n",
    "    txt = txt.lower()\n",
    "    ### special escaping character '...'\n",
    "    txt = txt.replace(u'\\u2026','.')\n",
    "    txt = txt.replace(u'\\u00a0',' ')\n",
    "    ### remove accent btw\n",
    "    txt = unicodedata.normalize('NFD', txt).encode('ascii', 'ignore')\n",
    "    ###txt = unidecode(txt)\n",
    "    ### remove non alphanumeric char\n",
    "    txt = re.sub('[^a-z_]', ' ', txt)\n",
    "    ### remove french stop words\n",
    "    tokens = [w for w in txt.split() if (len(w)>2) and (w not in stopwords)]\n",
    "    ### french stemming\n",
    "    tokens = [stemmer.stem(token) for token in tokens]\n",
    "    #Stemmers remove morphological affixes from words, leaving only the word stem\n",
    "    ### tokens = stemmer.stemWords(tokens)\n",
    "    return ' '.join(tokens)\n",
    "    #join() returns a string in which the string elements of sequence have been joined by str separator.\n",
    "\n",
    "def clean_marque(txt):\n",
    "    txt = re.sub('[^a-zA-Z0-9]', '_', txt).lower()\n",
    "    return txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage des DataFrames\n",
    "Applique le nettoyage sur toutes les lignes de la DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [
     1
    ]
   },
   "outputs": [],
   "source": [
    "# fonction de nettoyage du fichier(stemming et liste de mots à supprimer)\n",
    "def clean_df(input_data, column_names= ['Question','Intent','BlockId', 'Action']):\n",
    "    #Test if columns entry match columns names of input data\n",
    "    column_names_diff= set(column_names).difference(set(input_data.columns))\n",
    "    #set.difference   new set with elements in column_names but not in input_data.columns\n",
    "    \n",
    "    if column_names_diff: #rentre dans la boucle si column_names différent zero\n",
    "        # warning = exception\n",
    "        warnings.warn(\"Column(s) '\"+\", \".join(list(column_names_diff)) +\"' do(es) not match columns of input data\", Warning)\n",
    "        \n",
    "    nb_line = input_data.shape[0]\n",
    "    print(\"Start Clean %d lines\" %nb_line)\n",
    "    \n",
    "    # Cleaning start for each columns\n",
    "    time_start = time.time()\n",
    "    clean_list=[]\n",
    "    for column_name in column_names:\n",
    "        column = input_data[column_name].values\n",
    "        if column_name == \"Question\":\n",
    "            array_clean = np.array(map(clean_txt,column))\n",
    "            \n",
    "        elif column_name == \"Intent\":\n",
    "            array_clean = np.asarray(input_data['Intent']) #on recopie telle quelle la colonne intent  \n",
    "            \n",
    "        else:\n",
    "            array_clean = np.array(map(clean_marque,column))\n",
    "            #applies a function to all the items in an input_list\n",
    "            #map(function_to_apply, list_of_inputs)\n",
    "        clean_list.append(array_clean)\n",
    "    time_end = time.time()\n",
    "    print(\"Cleaning time: %d secondes\"%(time_end-time_start))\n",
    "    \n",
    "    #Convert list to DataFrame\n",
    "    array_clean = np.array(clean_list).T\n",
    "    data_clean = pd.DataFrame(array_clean, columns = column_names)\n",
    "    return data_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Clean 502 lines\n",
      "Cleaning time: 0 secondes\n",
      "Start Clean 4511 lines\n",
      "Cleaning time: 5 secondes\n"
     ]
    }
   ],
   "source": [
    "# Take approximately 2 minutes fors 100.000 rows\n",
    "data_valid_clean = clean_df(data_valid)\n",
    "data_train_clean = clean_df(data_train)\n",
    "#data_all_clean=clean_df(data_all) ne marche pas pourquoi ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affiche les 5 premières lignes de la DataFrame d'apprentissage après nettoyage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question</th>\n",
       "      <th>Intent</th>\n",
       "      <th>BlockId</th>\n",
       "      <th>Action</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>casqu refriger march</td>\n",
       "      <td>#2-42_QVDP_Alopecie_Casque</td>\n",
       "      <td>598435b6e4b03f0d12d99ffe</td>\n",
       "      <td>wiki_cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>canc sein saliv</td>\n",
       "      <td>#6-58_TRTEINS_Bouche_Seche</td>\n",
       "      <td>59918e6ee4b0feb2888c9bcd</td>\n",
       "      <td>wiki_cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>salon coiffur perruqu</td>\n",
       "      <td>#2-49_QVDP_Alopecie_OuPerruque</td>\n",
       "      <td>598439eae4b03f0d12f637ea</td>\n",
       "      <td>wiki_cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>souffr insomn malgr fatigu amelior</td>\n",
       "      <td>#2-64-0_QVDP_Fatigue</td>\n",
       "      <td>59942265e4b068eebf52bb9f</td>\n",
       "      <td>wiki_cancer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>drainag lymphat</td>\n",
       "      <td>#6-39_TRTEINS_DrainageLymphodeme</td>\n",
       "      <td>598b50d6e4b03f0d3717ad2d</td>\n",
       "      <td>wiki_cancer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Question                            Intent  \\\n",
       "0                casqu refriger march        #2-42_QVDP_Alopecie_Casque   \n",
       "1                     canc sein saliv        #6-58_TRTEINS_Bouche_Seche   \n",
       "2               salon coiffur perruqu    #2-49_QVDP_Alopecie_OuPerruque   \n",
       "3  souffr insomn malgr fatigu amelior              #2-64-0_QVDP_Fatigue   \n",
       "4                     drainag lymphat  #6-39_TRTEINS_DrainageLymphodeme   \n",
       "\n",
       "                    BlockId       Action  \n",
       "0  598435b6e4b03f0d12d99ffe  wiki_cancer  \n",
       "1  59918e6ee4b0feb2888c9bcd  wiki_cancer  \n",
       "2  598439eae4b03f0d12f637ea  wiki_cancer  \n",
       "3  59942265e4b068eebf52bb9f  wiki_cancer  \n",
       "4  598b50d6e4b03f0d3717ad2d  wiki_cancer  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nous allons concatener toutes les questions d'une même sous catégorie afin de n'avoir plus qu'une question par sous catégorie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['#6-49_TRTEINS_Peau',\n",
       " '#6-97_TRTEINS_Nausees_Vomissements',\n",
       " '#6-92_TRTEINS_Diarrhee',\n",
       " '#2-130_QVDP_Alimentation',\n",
       " '#6-53_TRTEINS_Aphtes',\n",
       " '#6-90_TRTEINS_Mauvais_Gout',\n",
       " '#6-98_TRTEINS_EI_Frequents',\n",
       " '#2-36_QVDP_Alopecie_Pourquoi',\n",
       " '#6-60_TRTEINS_PAC',\n",
       " '#9-2_Informations_cancer',\n",
       " '#6-18_TRTEINS_Radiotherapie',\n",
       " '#2-64-0_QVDP_Fatigue',\n",
       " '#6-57_TRTEINS_Yeux_Secs',\n",
       " '#2-55_QVDP_Douleur',\n",
       " 'conversation_rappel_rendezvous',\n",
       " '#6-96_TRTEINS_Perte_Poids',\n",
       " '#2-45_QVDP_Alopecie_Perruque',\n",
       " '#6-1_TRTEINS_Chimiotherapie',\n",
       " '#2-96_QVDP_Social_Priseencharge',\n",
       " '#6-58_TRTEINS_Bouche_Seche',\n",
       " '#6-24_TRTEINS_hormonotherapie',\n",
       " '#2-120_QVDP_Sexualite',\n",
       " '#2-107_QVDP_Grossesse',\n",
       " '#6-34_TRTEINS_Lymphoedeme',\n",
       " '#2-41_QVDP_Alopecie_Diminuer',\n",
       " '#6-73_TRTEINS_Chirurgie_Mastectomie',\n",
       " '#6-93_TRTEINS_Mauvaise_Haleine',\n",
       " '#2-128_QVDP_Sexualite_Couple',\n",
       " '#2-125_QVDP_Sexualite_Reconstruction',\n",
       " '#5-37_Soutien_Psychologique',\n",
       " '#2-73_QVDP_Sport',\n",
       " '#9-53_Informations_depistagesein',\n",
       " '#6-31_TRTEINS_Effetsecondaireshormonotherapie',\n",
       " '#6-95_TRTEINS_Perte_Appetit',\n",
       " '#2-115_QVDP_Grossesse_Allaitement',\n",
       " '#1-5_Menu_Aidant',\n",
       " '#2-38_QVDP_Alopecie_Repousse',\n",
       " '#6-41_TRTEINS_ManchonLymphodeme',\n",
       " '#6-65_TRTEINS_Chirurgie',\n",
       " '#001_Gestion_Abonnement',\n",
       " '#2-61_QVDP_RadioT_Peau',\n",
       " '#2-49_QVDP_Alopecie_OuPerruque',\n",
       " '#2-103_QVDP_Social_Banque',\n",
       " '#2-37_QVDP_Alopecie_Quand',\n",
       " '#2-48_QVDP_Alopecie_RbPerruque',\n",
       " '#2-87_QVDP_Fertilite',\n",
       " '#2-84_QVDP_Fertilite_GrossesseApresCancer',\n",
       " 'Conversation_rappel_read',\n",
       " '#2-70_QVDP_Fatigue_Diminuer',\n",
       " '#2-79_QVDP_SportQuel',\n",
       " '#2-90_QVDP_Fertilite_Ponction',\n",
       " '#7-52_Remission',\n",
       " '#2-53_QVDP_Cils',\n",
       " '#2-109_QVDP_Grossesse_Chimiotherapie',\n",
       " '#9-13_Informations_metastase',\n",
       " '#6-73_TRTEINS_Chirurgie_Curage_axillaire',\n",
       " '#2-126_QVDP_Sexualite_BaisseLibido',\n",
       " '#2-106_QVDP_Social_HAD',\n",
       " '#2-42_QVDP_Alopecie_Casque',\n",
       " '#2-58_QVDP_RadioT_Eff',\n",
       " '#2-52_QVDP_Ongles',\n",
       " '#2-133_QVDP_RegimeAnticancer',\n",
       " '#2-135_QVDP_Alimentation_Alcool',\n",
       " '#6-64_TRTEINS_PAC_Risques',\n",
       " '#6-83_TRTEINS_Prothese',\n",
       " '#2-72_QVDP_Fatigue_Sieste',\n",
       " '#2-97_QVDP_ResteACharge',\n",
       " '#6-40_TRTEINS_PressotherapieLymphodeme',\n",
       " '#2-47_QVDP_Alopecie_CoutPerruque',\n",
       " '#2-59_QVDP_RadioT_Fatigue',\n",
       " '#2-134_QVDP_Alimentation_Citron',\n",
       " '#9-20_Informations_Stades',\n",
       " '#2-60_QVDP_RadioT_Diarrh\\xc3\\x83\\xc2\\xa9e',\n",
       " '#2-110_QVDP_Grossesse_Radiotherapie',\n",
       " '#2-137_QVDP_Alimentation_Remboursement',\n",
       " '#2-111_QVDP_Grossesse_Chirurgie',\n",
       " 'Conversation_rappel_update2',\n",
       " '#6-12_TRTEINS_ChimioAmbulatoire',\n",
       " '#6-6_TRTEINS_ChimioOrale',\n",
       " '#2-93_QVDP_Social_Proches',\n",
       " '#6-72_TRTEINS_Chirurgie_Cicatrice',\n",
       " '#2-64_QVDP_Fatigue_Chimio',\n",
       " '#6-39_TRTEINS_DrainageLymphodeme',\n",
       " '#2-102_QVDP_Social_RbPerruque',\n",
       " '#2-121_QVDP_Sexualite_Cicatrice',\n",
       " '#2-56_QVDP_DiminuerDouleur',\n",
       " '#2-75_QVDP_SportPrecautions',\n",
       " '#6-13_TRTEINS_PreparationChimio',\n",
       " '#1-8_Menu_Traitement',\n",
       " '#2-101_QVDP_Social_RbProthese',\n",
       " '#2-65_QVDP_Fatigue_Hormono',\n",
       " '#6-22_TRTEINS_EffetsecondairesRadiotherapie',\n",
       " '#6-94_TRTEINS_Deglutition',\n",
       " '#6-23_TRTEINS_SeanceRadiotherapie',\n",
       " '#6-33_TRTEINS_Lymphe',\n",
       " '#2-123_QVDP_Sexualite_Prothese',\n",
       " '#6-63_TRTEINS_PAC_Precautions',\n",
       " '#2-118_QVDP_Grossesse_Malformations',\n",
       " '#6-9_TRTEINS_ChimioIntraperitoneale',\n",
       " '#2-122_QVDP_Sexualite_Alopecie',\n",
       " '#6-2_TRTEINS_Cycles',\n",
       " '#6-37_TRTEINS_TrtLymphodeme',\n",
       " '#7-58_Recidive',\n",
       " '#2-112_QVDP_Grossesse_Hormonotherapie',\n",
       " '#2-100_QVDP_Social_Transports',\n",
       " '#2-85_QVDP_Fertilite_Preserver',\n",
       " '#6-43_TRTEINS_PrixManchon',\n",
       " '#6-8_TRTEINS_Intrathecale',\n",
       " '#6-67_TRTEINS_Chirurgie_Questions',\n",
       " '#2-124_QVDP_Sexualite_PerteConfiance',\n",
       " 'Profile_read',\n",
       " '#000_Menu_General',\n",
       " '#6-91_TRTEINS_Ballonnement',\n",
       " '#8-6_DDP_Consentement_eclaire',\n",
       " '#7-49_QVDA_Personne_de_confiance',\n",
       " '#6-86_TRTEINS_Chirurgie_Reeducation',\n",
       " '#6-44_TRTEINS_AllergieManchon',\n",
       " '#9-53_Depistage_cancer_sein',\n",
       " '#6-35_TRTEINS_PrevenirLymphoedeme',\n",
       " '#1-4_Menu_Patient_Recent',\n",
       " '#8-8_DDP_Fin_de_vie',\n",
       " '#8-9_DDP_Charte_patient',\n",
       " 'Profile_update',\n",
       " '#2-88_Reprise_Cycle',\n",
       " '#6-81_TRTEINS_Chirurgie_Mastectomie_risques',\n",
       " 'Conversation_Historique',\n",
       " '#8-13_DDP_Secret_medical',\n",
       " '#2-88_QVDP_Fertilite_Cycles',\n",
       " '#8-11_DDP_Dossier_medical',\n",
       " '#7-60_QVDA_Soins_Palliatifs',\n",
       " 'Profile_write_age',\n",
       " '#6-5_TRTEINS_ChimioInjectable',\n",
       " '#6-68_TRTEINS_Chirurgie_Anesthesie',\n",
       " 'Profile_write_doseTraitement',\n",
       " '#6-47_TRTEINS_InfectionManchon',\n",
       " 'Conversation_rappel_update',\n",
       " 'Profile_write_typeTraitement',\n",
       " 'Profile_write_newsletter',\n",
       " 'Profile_write_rappels',\n",
       " 'Profile_write_nomTraitement',\n",
       " '#001_Abonnement',\n",
       " 'Profile_write_recevoirTemoignage',\n",
       " 'Profile_write_partagerTemoignage',\n",
       " 'Profile_write_typeCancer',\n",
       " 'Profile_write_role',\n",
       " 'intent']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "intents = pd.Series(data_all['Intent'])\n",
    "A=intents.value_counts(sort=True)\n",
    "list(A.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  soigner troubles du go\\xc3\\xbbt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame({'Question':[' ']*146, 'Intent':list(A.index)}, columns = [\"Question\", \"Intent\"])\n",
    "df2[df2['Intent']=='#6-53_TRTEINS_Aphtes'].values[0][0]\n",
    "df2[df2['Intent']=='#6-53_TRTEINS_Aphtes'].values[0][0]+' '+data_train.values[299][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alist=pd.DataFrame({'Intent':list(A.index), 'Numero': np.arange(146)}, columns = [ \"Intent\", \"Numero\"])\n",
    "alist[alist['Intent']=='#6-53_TRTEINS_Aphtes']['Numero'].values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "58",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-f990184d710f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mintent\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata_train_clean\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Intent'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0malist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0malist\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Intent'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mintent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Numero'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Intent\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mintent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf2\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"Intent\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mintent\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m##probleme d'affectation\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\emma\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2137\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2138\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2139\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2141\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\emma\\Anaconda2\\lib\\site-packages\\pandas\\core\\frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2144\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2145\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2146\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2148\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\emma\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1840\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1841\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1842\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1843\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1844\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\emma\\Anaconda2\\lib\\site-packages\\pandas\\core\\internals.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   3841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3842\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3843\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3844\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3845\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\emma\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexes\\base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   2525\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2526\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2527\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2528\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2529\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 58"
     ]
    }
   ],
   "source": [
    "for i in range (data_train.shape[0]):\n",
    "    intent=data_train_clean['Intent'][i]\n",
    "    df2[alist[alist['Intent']==intent]['Numero'].values[0]][df2[\"Intent\"]==intent]=df2[df2[\"Intent\"]==intent].values[0][0]+ ' ' +data_train.values[i][0] ##probleme d'affectation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "\n",
    "stop_words = nltk.corpus.stopwords.words('french')\n",
    "question = df2[\"Question\"] #on va travailler que sur la colonne des questions\n",
    "vec = TfidfVectorizer(\n",
    "            min_df = 1,\n",
    "            stop_words =stop_words,\n",
    "            smooth_idf=True,\n",
    "            norm='l2',\n",
    "            sublinear_tf=True,\n",
    "            use_idf=True, #tf avec idf\n",
    "            ngram_range=(1,2)) #bi-grams\n",
    "tfidf =  vec.fit_transform(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mot_disc2=dict(sorted(zip(vec.get_feature_names(),vec.idf_), key = lambda x: x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "from wordcloud import WordCloud\n",
    "description = df2[\"Question\"]\n",
    "description.to_csv('Question.csv', sep = ',')\n",
    "text = open(\"C:\\Users\\emma\\Documents\\ecole\\insa\\projet chabot\\Question.csv\").read()\n",
    "frequencies= mot_disc2 #dict word-freq\n",
    "# Generate a word cloud image\n",
    "wordcloud = WordCloud().generate_from_frequencies(frequencies)\n",
    "\n",
    "# Display the generated image:\n",
    "# the matplotlib way:\n",
    "#import matplotlib.pyplot as plt\n",
    "#plt.imshow(wordcloud, interpolation='bilinear')\n",
    "#plt.axis(\"off\")\n",
    "# lower max_font_size\n",
    "wordcloud = WordCloud(max_font_size=60).generate(text)\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# The pil way (if you don't have matplotlib)\n",
    "# image = wordcloud.to_image()\n",
    "# image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "nav_menu": {
    "height": "279px",
    "width": "252px"
   },
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
