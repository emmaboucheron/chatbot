{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "<a href=\"http://www.hupi.fr/\" ><img src=\"http://www.hupi.fr/wp-content/uploads/2016/03/hupi_logo_vectoris_menu.png\" style=\"float:right; max-width: 300px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies des données massives](https://github.com/wikistat/Ateliers-Big-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Text Mining* et Catégorisation des Produits Cdiscount avec [SparkML](https://spark.apache.org/docs/latest/ml-guide.html) de <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"R\"/></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le contenu de ce calepin est sensiblement identique au calepin précédent : Atelier-Cdiscount-pyspark.ipynb \n",
    "Dans ce dernier, le résultat de chaque étape était détaillé afin d'aider a la compréhension de celles-ci.  \n",
    "\n",
    "Dans ce calepin, nous utilisons la fonction **Pipeline** de la librairie spark-ML afin de créer un modèle qui inclut directement toutes les étapes, du nettoyage de texte jusqu'a l'apprantissage d'un modèle de regression logistique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.context.SparkContext at 0x7f1c61ff1710>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importation des packages génériques et ceux \n",
    "# des librairie ML et MLlib\n",
    "##Nettoyage\n",
    "import nltk\n",
    "import re\n",
    "##Liste\n",
    "from numpy import array\n",
    "##Temps\n",
    "import time\n",
    "##Row and Vector\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "##Hashage et vectorisation\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.feature import IDF\n",
    "##Regression logistique\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "##Decision Tree\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "##Random Forest\n",
    "from pyspark.ml.classification import RandomForestClassifier \n",
    "##Pour la création des DataFrames\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la base distribuée\n",
    "DATA_PATH=''\n",
    "data = sc.textFile(DATA_PATH+'Categorie_reduit.csv')\n",
    "\n",
    "# Remove header\n",
    "data_header = data.take(1)[0]\n",
    "lines = data.filter(lambda l : l!=data_header)\n",
    "# Split lines in cells\n",
    "split_lines = lines.map(lambda l : l.split(\";\"))\n",
    "# Rdd with ROW sql \n",
    "RowRDD = split_lines.map(lambda l : Row(categorie1 = l[0], description = l[3]+\" \"+l[4]))\n",
    "# transform RDD to DataFrame\n",
    "RowDF = RowRDD.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Extraction sous-échantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataTrain : size = 799334, DataTest : size = 190540\n"
     ]
    }
   ],
   "source": [
    "# Taux de sous-échantillonnage des données pour tester le programme de préparation\n",
    "# sur un petit jeu de données\n",
    "taux_donnees=[0.80,0.19,0.01]\n",
    "dataTrain, DataTest, data_drop = RowDF.randomSplit(taux_donnees)\n",
    "n_train = dataTrain.count()\n",
    "n_test= DataTest.count()\n",
    "print(\"DataTrain : size = %d, DataTest : size = %d\"%(n_train, n_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création du pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un Transformer pour l'étape de stemming.\n",
    "\n",
    "Dans le calepin précédent, nous avons définie une fonction stemmer à partir de la librairie *nltk*. Pour que celle-ci puisse être utilisé dans un **Pipeline ML**, nous devons en faire un objet **transformers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark import keyword_only\n",
    "from pyspark.ml import Transformer\n",
    "from pyspark.ml.param.shared import HasInputCol, HasOutputCol, Param\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "class MyNltkStemmer(Transformer, HasInputCol, HasOutputCol):\n",
    "\n",
    "    @keyword_only\n",
    "    def __init__(self, inputCol=None, outputCol=None):\n",
    "        super(MyNltkStemmer, self).__init__()\n",
    "        kwargs = self._input_kwargs\n",
    "        self.setParams(**kwargs)\n",
    "\n",
    "    @keyword_only\n",
    "    def setParams(self, inputCol=None, outputCol=None):\n",
    "        kwargs = self._input_kwargs\n",
    "        return self._set(**kwargs)\n",
    "\n",
    "    def _transform(self, dataset):\n",
    "        STEMMER = nltk.stem.SnowballStemmer('french')\n",
    "        def clean_text(tokens):\n",
    "            tokens_stem = [ STEMMER.stem(token) for token in tokens]\n",
    "            return tokens_stem\n",
    "        udfCleanText =  udf(lambda lt : clean_text(lt), ArrayType(StringType()))\n",
    "        out_col = self.getOutputCol()\n",
    "        in_col = dataset[self.getInputCol()]\n",
    "        return dataset.withColumn(out_col, udfCleanText(in_col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition des différentes étapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# liste des mots à supprimer\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words('french'))\n",
    "# Fonction tokenizer qui permet de remplacer un long texte par une liste de mot\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"description\", outputCol=\"tokenizedDescr\", pattern=\"[^a-z_]\",\n",
    "                                minTokenLength=3, gaps=True)\n",
    "\n",
    "#V1\n",
    "# Fonction StopWordsRemover qui permet de supprimer des mots\n",
    "#remover = StopWordsRemover(inputCol=\"tokenizedDescr\", outputCol=\"cleanDescr\", stopWords = list(STOPWORDS))\n",
    "\n",
    "#V2\n",
    "# Fonction StopWordsRemover qui permet de supprimer des mots\n",
    "remover = StopWordsRemover(inputCol=\"tokenizedDescr\", outputCol=\"stopTokenizedDescr\", stopWords = list(STOPWORDS))\n",
    "# Stemmer \n",
    "stemmer = MyNltkStemmer(inputCol=\"stopTokenizedDescr\", outputCol=\"cleanDescr\")\n",
    "\n",
    "# Indexer\n",
    "indexer = StringIndexer(inputCol=\"categorie1\", outputCol=\"categoryIndex\")\n",
    "\n",
    "# Hasing\n",
    "hashing_tf = HashingTF(inputCol=\"cleanDescr\", outputCol='tf', numFeatures=10000)\n",
    "\n",
    "# Inverse Document Frequency\n",
    "idf = IDF(inputCol=hashing_tf.getOutputCol(), outputCol=\"tfidf\")\n",
    "\n",
    "#Logistic Regression\n",
    "lr = LogisticRegression(maxIter=100, regParam=0.01, fitIntercept=False, tol=0.0001,\n",
    "            family = \"multinomial\", elasticNetParam=0.0, featuresCol=\"tfidf\", labelCol=\"categoryIndex\") #0 for L2 penalty, 1 for L1 penalty\n",
    "\n",
    "# Creation du pipeline\n",
    "pipeline = Pipeline(stages=[regexTokenizer, remover, stemmer, indexer, hashing_tf, idf, lr ])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimation du pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le paramètre de pénalisation (lasso) est pris par défaut sans optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR prend 280 s pour un echantillon d'apprentissage de taille : n = 799334\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "# On applique toutes les étapes sur la DataFrame d'apprentissage.\n",
    "model = pipeline.fit(dataTrain)\n",
    "time_end=time.time()\n",
    "time_lrm=(time_end - time_start)\n",
    "print(\"LR prend %d s pour un echantillon d'apprentissage de taille : n = %d\" %(time_lrm, n_train)) # (104s avec taux=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Estimation de l'erreur sur l'échantillon test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error = , pour un echantillon test de taille n = %d0.09570168993387218\n"
     ]
    }
   ],
   "source": [
    "predictionsDF = model.transform(DataTest)\n",
    "labelsAndPredictions = predictionsDF.select(\"categoryIndex\",\"prediction\").collect()\n",
    "nb_good_prediction = sum([r[0]==r[1] for r in labelsAndPredictions])\n",
    "testErr = 1-nb_good_prediction/n_test\n",
    "print('Test Error = , pour un echantillon test de taille n = %d' + str(testErr)) # (0.08 avec taux =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Taille M| Temps | Erreur\n",
    "-------|-------|--------\n",
    "1.131  | 786   | 0.94"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
