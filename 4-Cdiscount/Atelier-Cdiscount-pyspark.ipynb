{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<a href=\"http://www.insa-toulouse.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/logo-insa.jpg\" style=\"float:left; max-width: 120px; display: inline\" alt=\"INSA\"/></a> \n",
    "\n",
    "<a href=\"http://wikistat.fr/\" ><img src=\"http://www.math.univ-toulouse.fr/~besse/Wikistat/Images/wikistat.jpg\" style=\"max-width: 250px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "<a href=\"http://www.hupi.fr/\" ><img src=\"http://www.hupi.fr/wp-content/uploads/2016/03/hupi_logo_vectoris_menu.png\" style=\"float:right; max-width: 300px; display: inline\"  alt=\"Wikistat\"/></a>\n",
    "\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Ateliers: Technologies des données massives](https://github.com/wikistat/Ateliers-Big-Data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Text Mining* et Catégorisation des Produits Cdiscount avec [SparkML](https://spark.apache.org/docs/latest/ml-guide.html) de <a href=\"http://spark.apache.org/\"><img src=\"http://spark.apache.org/images/spark-logo-trademark.png\" style=\"max-width: 100px; display: inline\" alt=\"R\"/></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il s'agit d'une version simplifiée du concours proposé par Cdiscount et paru sur le site [datascience.net](https://www.datascience.net/fr/challenge). Les données d'apprentissage sont accessibles sur demande auprès de Cdiscount mais les solutions de l'échantillon test du concours ne sont pas et ne seront pas rendues publiques. Un échantillon test est donc construit pour l'usage de ce tutoriel.  L'objectif est de prévoir la catégorie d'un produit à partir de son descriptif. Seule la catégorie principale (1er niveau, 47 classes) est prédite au lieu des trois niveaux demandés dans le concours. L'objectif est plutôt de comparer les performances des méthodes et technologies en fonction de la taille de la base d'apprentissage ainsi que d'illustrer sur un exemple complexe le prétraitement de données textuelles. La stratégie de sous ou sur échantillonnage des catégories qui permet d'améliorer la prévision n'a pas été mise en oeuvre.\n",
    "* L'exemple est présenté avec la possibilité de sous-échantillonner afin de réduire les temps de calcul. \n",
    "* L'échantillon réduit peut encore l'être puis, après \"nettoyage\", séparé en 2 parties: apprentissage et test. \n",
    "* Les données textuelles de l'échantillon d'apprentissage sont, \"racinisées\", \"hashées\", \"vectorisées\" avant modélisation.\n",
    "* Les mêmes transformations, notamment (hashage et TF-IDF) évaluées sur l'échantillon d'apprentissage sont appliquées à l'échantillon test.\n",
    "* Un seul modèle est estimé par régression logistique \"multimodal\", plus précisément et implicitement, un modèle par classe.\n",
    "* Différents paramètres: de vectorisation (hashage, TF-IDF), paramètres de la régression logistique (pénalisation L1) pourraient encore être optimisés.\n",
    "\n",
    "Ce calepin utilise l'API pySpark pour traiter des données volumineuses distribuées. Les modélisations sont obtenues à l'aide de la librairies [ML](https://spark.apache.org/docs/latest/ml-guide.html).\n",
    "\n",
    "Un autre [calepin](http://www.math.univ-toulouse.fr/~besse/Wikistat/Notebooks/Tuto_Notebook_Cdiscount.html) utilise un programme python et la librairie [scikit-learn](http://scikit-learn.org/stable/). L'objectif est de comparer les performances: temps de nettoyage des données et d'apprentissage, erreurs de prévision, dans les deux environnements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Importation des packages génériques et ceux \n",
    "# des librairie ML et MLlib\n",
    "##Nettoyage\n",
    "import nltk\n",
    "import re\n",
    "##Liste\n",
    "from numpy import array\n",
    "##Temps\n",
    "import time\n",
    "##Row and Vector\n",
    "from pyspark.sql import Row\n",
    "from pyspark.ml.linalg import Vectors\n",
    "##Hashage et vectorisation\n",
    "from pyspark.ml.feature import HashingTF\n",
    "from pyspark.ml.feature import IDF\n",
    "##Regression logistique\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "##Decision Tree\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "##Random Forest\n",
    "from pyspark.ml.classification import RandomForestClassifier \n",
    "##Pour la création des DataFrames\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nettoyage des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Lecture des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de la base distribuée\n",
    "DATA_PATH=''\n",
    "data = sc.textFile(DATA_PATH+'Cdiscount_reduit.csv')\n",
    "\n",
    "# Remove header\n",
    "data_header = data.take(1)[0]\n",
    "lines = data.filter(lambda l : l!=data_header)\n",
    "# Split lines in cells\n",
    "split_lines = lines.map(lambda l : l.split(\";\"))\n",
    "# Rdd with ROW sql \n",
    "RowRDD = split_lines.map(lambda l : Row(categorie1 = l[0], description = l[3]+\" \"+l[4]))\n",
    "# transform RDD to DataFrame\n",
    "RowDF = RowRDD.toDF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Extraction sous-échantillon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taux de sous-échantillonnage des données pour tester le programme de préparation\n",
    "# sur un petit jeu de données\n",
    "taux_donnees=0.99\n",
    "dataEchDF,data_drop = RowDF.randomSplit([taux_donnees,1-taux_donnees])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les instructions précédentes ne sont pas exécutées (exécution paresseuse ou *lazy*). Elles ne le sont que lors d'un traitement explicite."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=time.time()\n",
    "size = dataEchDF.count()\n",
    "te=time.time()\n",
    "rt_count = te-ts\n",
    "print(\"Comptage prend %d s, pour une taille de %d\" %(rt_count, size)) #63s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "taux | Taille M | Temps\n",
    "-----|--------|------\n",
    "0.01 | 0.157  | 51\n",
    "0.1  | 1.57   | 49\n",
    "0.4  | 6.29   | 53\n",
    "0.8  | 12.6   | 60\n",
    " 1   | 15.7   | 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nettoyage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de limiter la dimension de l'espace des variables ou *features* tout en conservant les informations essentielles, il est nécessaire de nettoyer les données en appliquant plusieurs étapes:\n",
    "* **Tokenizer**: Les textes de descrpitions sont transformés en liste de mots. Chaque mot est écrit en minuscule, les termes numériques, de ponctuation et autres symboles sont supprimés.\n",
    "* **StopWord**: 155 mots-courants, et donc non informatif, de la langue française sont supprimés (STOPWORDS). Ex: le, la, du, alors, etc...\n",
    "* **Stemmer**: Chaque mot est \"racinisé\". La racinisation transforme un mot en son radical ou sa racine. Par exemple, les mots: cheval, chevaux, chevalier, chevalerie, chevaucher sont tous remplacés par \"cheva\".\n",
    "\n",
    "La librairie pyspark.ml ne supporte actuellement pas de fonction de stemming pour pyspark. Pour cette étape nous faisons donc appel à la librairie nltk de python et la fonction *nltk.stem.SnowballStemmer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from pyspark.sql.types import ArrayType\n",
    "from pyspark.sql.functions import udf,col\n",
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover\n",
    "\n",
    "# liste des mots à supprimer\n",
    "STOPWORDS = set(nltk.corpus.stopwords.words('french'))\n",
    "# Fonction tokenizer qui permet de remplacer un long texte par une liste de mot\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"description\", outputCol=\"tokenizedDescr\", pattern=\"[^a-z_]\",\n",
    "                                minTokenLength=3, gaps=True)\n",
    "dataTokenized = regexTokenizer.transform(dataEchDF)\n",
    "\n",
    "# Fonction StopWordsRemover qui permet de supprimer des mots\n",
    "remover = StopWordsRemover(inputCol=\"tokenizedDescr\", outputCol=\"tokenizedRemovedDescr\", stopWords = list(STOPWORDS))\n",
    "dataTokenizedRemoved = remover.transform(dataTokenized)\n",
    "\n",
    "# La fonction de stemming de spark existe aujourd'hui qu'en scala et ne possède pas de wrapper en python.\n",
    "# Cette étape doit donc être effectué en utilisant des librairies python et notamment nltk\n",
    "STEMMER = nltk.stem.SnowballStemmer('french')\n",
    "\n",
    "def clean_text(tokens):\n",
    "    tokens_stem = [ STEMMER.stem(token) for token in tokens]\n",
    "    return tokens_stem\n",
    "udfCleanText =  udf(lambda lt : clean_text(lt), ArrayType(StringType()))\n",
    "dataClean = dataTokenizedRemoved.withColumn(\"cleanDescr\", udfCleanText(col('tokenizedRemovedDescr')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataClean.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new rdd with the resulte of the clean_text function on the description\n",
    "ts=time.time()\n",
    "size = dataClean.count()\n",
    "te=time.time()\n",
    "rt = te-ts\n",
    "print(\"Nettoyage prend %d s, pour la taille %d\" %(rt, size)) #64s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction des Labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convertir les 47 labels d'origine (Catégorie 1), qui ne sont pas dans un format acceptable (string) par les fonctions de sparkML, en des entiers numérotés de 0 a 46 nécessaires lors de l'application de la régression logistique. \n",
    "Cette étape peut être facilement réalisé a l'aide de la fonction *StringIndexer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=time.time()\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "indexer = StringIndexer(inputCol=\"categorie1\", outputCol=\"categoryIndex\")\n",
    "dataCleanindexed = indexer.fit(dataClean).transform(dataClean)\n",
    "te=time.time()\n",
    "rt = te-ts\n",
    "print(\"Index car prend %d s, taille %d\" %(rt, size)) #67"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création des DataFrame train and Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une fois extrait l'échantillon test, l'échantillon d'apprentissage peut être sous échantillonné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tauxEch=0.8\n",
    "(trainTotDF, testDF) = dataCleanindexed.randomSplit([tauxEch, 1-tauxEch])\n",
    "n_test = trainTotDF.count()\n",
    "print(n_test)  # 314441\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sous-échantillonnage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si toutes les données sont préparées, il est possible de sous-échantillonner l'échantillon d'apprentissage pour étudier l'impact de sa taille sur l'erreur de prévision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tauxApp=0.9\n",
    "(trainDF, testDF)=trainTotDF.randomSplit([tauxApp, 1-tauxApp])\n",
    "n_train=trainDF.count()\n",
    "print(n_train)  # 1131577"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Construction des *features* (TF-IDF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La vectorisation, c'est à dire la construction des *features* à partir de la liste des mots se fait en 2 étapes:\n",
    "* Le hashage permet de réduire l'espace des variables (taille du dictionnaire) en un nombre limité et fixé a priori *n_hash* de *features*. Il repose sur la définition d'une *hash function*, $h$ qui à un indice $j$ défini dans l'espace des entiers naturels, renvoie un indice $i=h(j)$ dans dans l'espace réduit (1 à *n_hash*) des *features*. Ainsi le poids de l'indice $i$, du nouvel espace, est l'association de tous les poids d'indice $j$ tels que $i=h(j)$ de l'espace originale. Ici, les poids sont associés d'après la méthode décrit par [Weinberger et al. (2009)](https://arxiv.org/pdf/0902.2206.pdf). \n",
    "\n",
    "*N.B.* *$h$ n'est pas généré aléatoirement. Ainsi pour un même fichier d'apprentissage (ou de test) et pour un même entier n_hash, le résultat de la fonction de hashage est identique*  \n",
    "\n",
    "* Le *TF-IDF*. Le TF-IDF  permet de faire ressortir l'importance relative de chaque mot $m$ (ou couples de mots consécutifs) dans un texte-produit ou un descriptif $d$, par rapport à la liste entière des produits. La fonction $TF(m,d)$ compte le nombre d'occurences du mot $m$ dans le descriptif $d$. La fonction $IDF(m)$  mesure l'importance du terme dans l'ensemble des documents ou descriptifs en donnant plus de poids aux termes les moins fréquents car considérés comme les plus discriminants (motivation analogue à celle de la métrique du chi2 en anamlyse des correspondance). $IDF(m,l)=\\log\\frac{D}{f(m)}$ où $D$ est le nombre de documents, la taille de l'échantillon d'apprentissage, et $f(m)$ le nombre de documents ou descriptifs contenant le mot $m$. La nouvelle variable ou *features* est $V_m(l)=TF(m,l)\\times IDF(m,l)$. \n",
    "\n",
    "* Comme pour les transformations des variables quantitatives (centrage, réduction), la même transformation c'est-à-dire les mêmes pondérations, est calculée sur l'achantillon d'apprentissage et appliquée à celui de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Traitement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le traitement qui suit, étape de hashage et calcul de $TF$ sont associés dans la même fonction.\n",
    "\n",
    "Les transformations *hashage* et *tf-idf*  calculées sur l'apprentissage sont appliquées à  l'échantillon test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDF.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts=time.time()\n",
    "# Term Frequency\n",
    "hashing_tf = HashingTF(inputCol=\"cleanDescr\", outputCol='tf', numFeatures=10000)\n",
    "trainTfDF = hashing_tf.transform(trainDF)\n",
    "# Inverse Document Frequency\n",
    "idf = IDF(inputCol=hashing_tf.getOutputCol(), outputCol=\"tfidf\")\n",
    "idf_model = idf.fit(trainTfDF) \n",
    "trainTfIdfDF = idf_model.transform(trainTfDF)\n",
    "# application à l'échantillon tesy\n",
    "testTfDF = hashing_tf.transform(testDF)\n",
    "testTfIdfDF = idf_model.transform(testTfDF)\n",
    "te=time.time()\n",
    "rt = te-ts\n",
    "print(\"Hashage et vectorisation prennent %d s, taille %d\" %(rt, n_train))  #456 s, taille 1131577"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Modélisation et test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le [calepin](http://www.math.univ-toulouse.fr/~besse/Wikistat/Notebooks/Tuto_Notebook_Cdiscount.html) python a clairement montré la meilleure performance de la régression logistique dans ce problème. C'est d'ailleurs ce modèle ou une *pyramide de logistiques* qui est industriellement implémentée."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Estimation du modèle logistique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le paramètre de pénalisation (lasso) est pris par défaut sans optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Configuraiton des paramètres de la méthode\n",
    "time_start=time.time()\n",
    "lr = LogisticRegression(maxIter=100, regParam=0.01, fitIntercept=False, tol=0.0001,\n",
    "            family = \"multinomial\", elasticNetParam=0.0, featuresCol=\"tfidf\", labelCol=\"categoryIndex\") #0 for L2 penalty, 1 for L1 penalty\n",
    "\n",
    "### Génération du modèle\n",
    "model_lr = lr.fit(trainTfIdfDF)\n",
    " \n",
    "time_end=time.time()\n",
    "time_lrm=(time_end - time_start)\n",
    "print(\"LR prend %d s\" %(time_lrm)) # (104s avec taux=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Estimation de l'erreur sur l'échantillon test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictionsDF = model_lr.transform(testTfIdfDF)\n",
    "labelsAndPredictions = predictionsDF.select(\"categoryIndex\",\"prediction\").collect()\n",
    "nb_good_prediction = sum([r[0]==r[1] for r in labelsAndPredictions])\n",
    "testErr = 1-nb_good_prediction/n_test\n",
    "print('Test Error = ' + str(testErr)) # (0.08 avec taux =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Taille M| Temps | Erreur\n",
    "-------|-------|--------\n",
    "1.131  | 786   | 0.94"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
